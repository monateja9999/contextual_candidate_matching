{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING GROQ API TO GENERATE JOB DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from groq import Groq\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_api_key = 'YOUR_API_KEY' #API_KEY is removed, in order to run the code, replace the valid groq api_key\n",
    "\n",
    "client = Groq(\n",
    "    api_key=grok_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_required = '''generate a 200 word job description for this resume without proper nouns \n",
    "The job description should only include and do not include statements like 'Here is a 200-word job description for the provided resume:': \n",
    "A Role summary summarizing the role's responsibilities and objectives\n",
    "Responsibilities, which should include specific tasks and duties related to the role\n",
    "Required qualifications, which should include education, certifications, and experience\n",
    "Desired qualifications, which should include additional education, certifications, and experiences that would be beneficial for the role\n",
    "Additional requirements, which should include any special requirements or constraints related to the role \n",
    "'''\n",
    "\n",
    "def get_prompt(resume):\n",
    "    return f'{prompt_required} \"{resume}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens 2136 total_tokens 2136\n",
      "1\n",
      "tokens 920 total_tokens 3056\n",
      "2\n",
      "tokens 1305 total_tokens 4361\n",
      "3\n",
      "tokens 1210 total_tokens 5571\n",
      "4\n",
      "tokens 1463 total_tokens 7034\n",
      "5\n",
      "tokens 4182 total_tokens 11216\n",
      "6\n",
      "tokens 3964 total_tokens 15180\n",
      "7\n",
      "tokens 1423 total_tokens 16603\n",
      "8\n",
      "tokens 1589 total_tokens 18192\n",
      "9\n",
      "tokens 1972 total_tokens 20164\n",
      "10\n",
      "tokens 3378 total_tokens 23542\n",
      "11\n",
      "tokens 1788 total_tokens 25330\n",
      "12\n",
      "tokens 2003 total_tokens 27333\n",
      "13\n",
      "tokens 873 total_tokens 28206\n",
      "14\n",
      "tokens 2342 total_tokens 30548\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary to store the row data of resume corpus in the required format\n",
    "data = {\"resume\": [], \"categories\": [], \"job_description\": []}\n",
    "\n",
    "#create a dataframe and keep adding the rows to dataframe for each file we process\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the directory\n",
    "dir_path = \"./data/resumes_corpus\"\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "files = os.listdir(dir_path)\n",
    "\n",
    "#check the limit of the grop_api and count the number of tokens used\n",
    "total_tokens = 0\n",
    "\n",
    "#set the number of files for the training data\n",
    "num_files = 0\n",
    "\n",
    "\n",
    "# Iterate over each file\n",
    "for file in files:\n",
    "\n",
    "    if file.endswith(\".txt\"):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(dir_path, file)\n",
    "        file_path_label = file_path.replace(\".txt\", \".lab\")\n",
    "        data = {\"resume\": [], \"categories\": [], \"job_description\": []}\n",
    "\n",
    "        # print(\"file_path\",file_path)\n",
    "        # print(\"file_path_label\",file_path_label)\n",
    "\n",
    "\n",
    "        # Open the file\n",
    "        with codecs.open(file_path, \"r\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "            # Read the file's contents\n",
    "            resume = f.read()\n",
    "            num_files += 1\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": get_prompt(resume)}],\n",
    "                temperature=1,\n",
    "                max_tokens=1024,\n",
    "                top_p=1,\n",
    "                stream=False,\n",
    "                stop=None,\n",
    "            )\n",
    "            message_content = response.choices[0].message.content\n",
    "            # print(message_content)\n",
    "            tokens = response.usage.total_tokens\n",
    "            total_tokens += tokens\n",
    "            \n",
    "            data[\"resume\"].append(resume)\n",
    "            data[\"job_description\"].append(message_content)\n",
    "            print(\"tokens\",tokens, 'total_tokens',total_tokens)\n",
    "\n",
    "\n",
    "        with codecs.open(file_path_label, \"r\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "            # Read the file's contents\n",
    "            content = f.read()\n",
    "            content = content.split(\"\\n\")\n",
    "            if \"\" in content:\n",
    "                content.remove(\"\")\n",
    "            data[\"categories\"].append(content)\n",
    "\n",
    "        new_row_df = pd.DataFrame(data)\n",
    "        # Add the new row to the DataFrame\n",
    "        df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "        \n",
    "\n",
    "        #rate limiter\n",
    "        if total_tokens > 55000:\n",
    "            total_tokens = 0\n",
    "            df.to_json(\"./data_sample/data.json\", orient=\"records\", indent=4)\n",
    "            # time.sleep(10)\n",
    "\n",
    "        print(num_files)\n",
    "        if num_files == 15:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"./data_sample/data.json\", orient=\"records\", indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlptf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
